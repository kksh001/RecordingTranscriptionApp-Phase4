import Foundation
import AVFoundation
import Speech
import Combine

// MARK: - Real-time Transcription Manager
class RealTimeTranscriptionManager: NSObject, ObservableObject {
    
    // MARK: - Published Properties
    @Published var isTranscribing = false
    @Published var isPaused = false
    @Published var currentTranscript = ""
    @Published var realtimeSegments: [RealtimeSegment] = []
    @Published var transcriptionError: String?
    @Published var transcriptionQuality: TranscriptionQuality = .unknown
    @Published var detectedLanguage: String = "en-US"
    @Published var confidence: Float = 0.0
    
    // MARK: - Configuration
    @Published var enableRealTimeTranscription = true
    @Published var transcriptionBufferSize: TimeInterval = 10.0
    @Published var qualityThreshold: Float = 0.7
    
    // MARK: - Private Properties
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine = AVAudioEngine()
    
    // Timer and state management
    private var transcriptionTimer: Timer?
    private var segmentTimer: Timer?
    private var lastTranscriptionTime = Date()
    private var currentSegmentStartTime = Date()
    private var segmentInterval: TimeInterval = 8.0 // åˆç†çš„æ£€æŸ¥é—´éš”
    private var maxSegmentDuration: TimeInterval = 20.0 // å…è®¸æ›´é•¿çš„è¯­ä¹‰å®Œæ•´åˆ†æ®µ
    private var minSegmentDuration: TimeInterval = 5.0 // é¿å…è¿‡çŸ­åˆ†æ®µ
    
    // Language support
    private let supportedLanguages = ["en-US", "zh-CN", "es-ES", "fr-FR", "de-DE", "ja-JP"]
    
    // è¯­ä¹‰åˆ‡åˆ†ç›¸å…³
    private var lastTranscriptLength: Int = 0
    private var noChangeCounter: Int = 0
    private var lastSignificantUpdate: Date = Date()
    private var pendingTranscript: String = ""
    
    // ğŸ“ å·²å¤„ç†æ–‡æœ¬ä½ç½®è·Ÿè¸ª
    private var lastProcessedLength: Int = 0
    
    // MARK: - Industry Best Practice: Hybrid Segmentation Strategy
    
    // é…ç½®å‚æ•° - åŸºäºGoogle E2E Segmenterå’ŒMetaç ”ç©¶
    private var windowSize: TimeInterval = 12.0 // ä¸»è¦çª—å£å¤§å°
    private var strideLength: TimeInterval = 2.0 // é‡å æ­¥é•¿
    private var minSegmentLength: TimeInterval = 5.0 // åˆç†çš„æœ€å°æ®µè½é•¿åº¦
    private var maxSegmentLength: TimeInterval = 25.0 // å…è®¸æ›´é•¿çš„è¯­ä¹‰å®Œæ•´æ®µè½
    
    // å¤šä¿¡å·æ£€æµ‹çŠ¶æ€
    private var acousticBoundaryDetected: Bool = false
    private var semanticBoundaryDetected: Bool = false
    private var currentWindowBuffer: String = ""
    private var strideBuffer: String = ""
    private var lastBoundaryTime: Date = Date()
    
    // æ»‘åŠ¨çª—å£ç¼“å†²åŒº - å‚è€ƒWav2Vec2æœ€ä½³å®è·µ
    private var audioSegmentBuffer: [(text: String, timestamp: Date, confidence: Float)] = []
    private var processingWindow: (start: Date, end: Date)?
    
    override init() {
        super.init()
        setupSpeechRecognition()
    }
    
    deinit {
        stopRealTimeTranscription()
    }
    
    // MARK: - Setup
    private func setupSpeechRecognition() {
        // åˆå§‹è¯­è¨€è®¾ç½®ä¸ºç³»ç»Ÿè¯­è¨€æˆ–è‹±è¯­
        let locale = Locale.current.language.languageCode?.identifier ?? "en"
        let speechLocale = mapToSpeechLocale(locale)
        detectedLanguage = speechLocale
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: speechLocale))
        speechRecognizer?.delegate = self
    }
    
    private func mapToSpeechLocale(_ languageCode: String) -> String {
        switch languageCode {
        case "en": return "en-US"
        case "zh": return "zh-CN"
        case "es": return "es-ES"
        case "fr": return "fr-FR"
        case "de": return "de-DE"
        case "ja": return "ja-JP"
        default: return "en-US"
        }
    }
    
    // MARK: - Permission Management
    func requestTranscriptionPermission() async -> Bool {
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                DispatchQueue.main.async {
                    continuation.resume(returning: status == .authorized)
                }
            }
        }
    }
    
    // MARK: - Real-time Transcription Control
    func startRealTimeTranscription() async throws {
        print("ğŸ¤ Starting real-time transcription...")
        
        guard enableRealTimeTranscription else { 
            print("âŒ Real-time transcription disabled")
            return 
        }
        
        // æ£€æŸ¥æƒé™
        let hasPermission = await requestTranscriptionPermission()
        guard hasPermission else {
            DispatchQueue.main.async {
                self.transcriptionError = "Speech recognition permission denied"
                print("âŒ Speech recognition permission denied")
            }
            return
        }
        
        // æ£€æŸ¥è¯­éŸ³è¯†åˆ«å¯ç”¨æ€§
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            DispatchQueue.main.async {
                self.transcriptionError = "Speech recognition not available"
                print("âŒ Speech recognition not available")
            }
            return
        }
        
        try startAudioEngine()
        setupRecognitionRequest()
        
        DispatchQueue.main.async {
            self.isTranscribing = true
            self.transcriptionError = nil
            self.currentTranscript = ""
            self.realtimeSegments.removeAll()
            self.currentSegmentStartTime = Date()
            
            // ğŸ”¥ é‡ç½®æ–‡æœ¬å¤„ç†çŠ¶æ€
            self.lastProcessedLength = 0
            self.lastTranscriptLength = 0
            self.lastSignificantUpdate = Date()
            
            // é‡ç½®æ··åˆåˆ†å‰²çŠ¶æ€
            self.acousticBoundaryDetected = false
            self.semanticBoundaryDetected = false
            self.currentWindowBuffer = ""
            self.strideBuffer = ""
            self.lastBoundaryTime = Date()
            self.audioSegmentBuffer.removeAll()
            self.processingWindow = nil
            
            print("âœ… Real-time transcription started successfully")
        }
        
        startTranscriptionTimer()
        startSegmentTimer()
    }
    
    func stopRealTimeTranscription() {
        print("ğŸ›‘ Stopping real-time transcription...")
        
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        transcriptionTimer?.invalidate()
        segmentTimer?.invalidate()
        
        DispatchQueue.main.async {
            self.isTranscribing = false
            self.isPaused = false
            self.recognitionRequest = nil
            self.recognitionTask = nil
            
            // ğŸ§  è‡ªåŠ¨æ‰§è¡ŒAIé©±åŠ¨çš„æ™ºèƒ½åˆ†æ®µä¼˜åŒ–
            print("ğŸ§  Performing AI-driven segment optimization...")
            Task {
                await self.performAISegmentOptimization()
            }
            
            print("âœ… Real-time transcription stopped")
            print("ğŸ“Š Final segments count: \(self.realtimeSegments.count)")
        }
    }
    
    func pauseRealTimeTranscription() {
        print("â¸ï¸ Pausing real-time transcription...")
        audioEngine.pause()
        transcriptionTimer?.invalidate()
        segmentTimer?.invalidate()
        
        DispatchQueue.main.async {
            self.isTranscribing = false
            self.isPaused = true
        }
    }
    
    func resumeRealTimeTranscription() throws {
        print("â–¶ï¸ Resuming real-time transcription...")
        try audioEngine.start()
        startTranscriptionTimer()
        startSegmentTimer()
        
        DispatchQueue.main.async {
            self.isTranscribing = true
            self.isPaused = false
        }
    }
    
    // MARK: - Audio Engine Setup
    private func startAudioEngine() throws {
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // ç§»é™¤ä¹‹å‰çš„tapï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        inputNode.removeTap(onBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            self.recognitionRequest?.append(buffer)
            self.processAudioBuffer(buffer)
        }
        
        audioEngine.prepare()
        try audioEngine.start()
        
        print("ğŸµ Audio engine started successfully")
    }
    
    private func setupRecognitionRequest() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.requiresOnDeviceRecognition = false // ä½¿ç”¨äº‘ç«¯è¯†åˆ«è·å¾—æ›´å¥½è´¨é‡
        
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        guard let speechRecognizer = speechRecognizer else { return }
        
        print("ğŸ” Setting up speech recognition task...")
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { result, error in
            DispatchQueue.main.async {
                if let result = result {
                    self.processTranscriptionResult(result)
                }
                
                if let error = error {
                    print("âŒ Speech recognition error: \(error.localizedDescription)")
                    self.transcriptionError = error.localizedDescription
                    // ä¸è¦ç«‹å³åœæ­¢ï¼Œå…è®¸ç»§ç»­å°è¯•
                }
            }
        }
    }
    
    // MARK: - Audio Processing
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        // ç®€åŒ–çš„éŸ³é¢‘è´¨é‡åˆ†æ
        let audioQuality = analyzeSimpleAudioQuality(buffer)
        
        DispatchQueue.main.async {
            self.updateAudioQualityMetrics(audioQuality)
        }
    }
    
    private func analyzeSimpleAudioQuality(_ buffer: AVAudioPCMBuffer) -> AudioQualityMetrics {
        guard let channelData = buffer.floatChannelData?[0] else {
            return AudioQualityMetrics(signalToNoiseRatio: 0, averageAmplitude: 0, peakAmplitude: 0, spectralCentroid: 0, timestamp: Date())
        }
        
        let frameLength = Int(buffer.frameLength)
        var sum: Float = 0
        var peak: Float = 0
        
        for i in 0..<frameLength {
            let sample = abs(channelData[i])
            sum += sample
            peak = max(peak, sample)
        }
        
        let average = frameLength > 0 ? sum / Float(frameLength) : 0
        let snr = peak > 0 ? 20 * log10(peak / (average + 0.001)) : 0
        
        return AudioQualityMetrics(
            signalToNoiseRatio: snr,
            averageAmplitude: average,
            peakAmplitude: peak,
            spectralCentroid: 0, // ç®€åŒ–å®ç°
            timestamp: Date()
        )
    }
    
    // MARK: - Transcription Processing
    private func processTranscriptionResult(_ result: SFSpeechRecognitionResult) {
        // ğŸ”¥ ç®€åŒ–ï¼šåªä½¿ç”¨è¯­ä¹‰åˆ‡åˆ†ï¼Œç¦ç”¨æ··åˆåˆ†å‰²é¿å…å†²çª
        let transcriptText = result.bestTranscription.formattedString
        currentTranscript = transcriptText
        
        // æ›´æ–°ç½®ä¿¡åº¦å’Œè´¨é‡
        let avgConfidence = result.bestTranscription.segments.map { $0.confidence }.reduce(0, +) / Float(result.bestTranscription.segments.count)
        confidence = avgConfidence
        transcriptionQuality = determineQualityFromConfidence(avgConfidence)
        
        // ğŸ”¥ ä½¿ç”¨æ”¹è¿›çš„è¯­ä¹‰åˆ‡åˆ†
        checkForSemanticSegmentation(transcriptText, isFinal: result.isFinal)
        
        print("ğŸ“ Live transcription: '\(currentTranscript)' (confidence: \(confidence))")
    }
    
    // æ™ºèƒ½è¯­ä¹‰åˆ‡åˆ†æ£€æµ‹
    private func checkForSemanticSegmentation(_ currentText: String, isFinal: Bool) {
        let currentLength = currentText.count
        let lengthChanged = currentLength != lastTranscriptLength
        
        if lengthChanged {
            lastSignificantUpdate = Date()
            noChangeCounter = 0
            lastTranscriptLength = currentLength
            
            // ğŸ”¥ çœŸæ­£çš„è¯­ä¹‰åˆ‡åˆ†ï¼šæ¯æ¬¡æ–‡æœ¬å˜åŒ–æ—¶æ£€æŸ¥è¯­ä¹‰å®Œæ•´æ€§
            checkForSemanticCompletion(currentText, isFinal: isFinal)
        } else {
            noChangeCounter += 1
            
            // è¾…åŠ©ï¼šæ£€æŸ¥åœé¡¿åçš„å¼ºåˆ¶åˆ‡åˆ†
            checkForPauseBasedSegmentation(currentText)
        }
    }
    
    // è¯­ä¹‰å®Œæ•´æ€§æ£€æµ‹ï¼ˆå³æ—¶è§¦å‘ï¼Œä¸ç­‰å¾…åœé¡¿ï¼‰
    private func checkForSemanticCompletion(_ currentText: String, isFinal: Bool) {
        var shouldCreateSegment = false
        var segmentReason = ""
        
        // ä¸¥æ ¼çš„æœ€å°å†…å®¹é•¿åº¦è¦æ±‚ - é˜²æ­¢é›¶ç¢åˆ†æ®µ
        let minContentLength = 100 // è‡³å°‘100ä¸ªå­—ç¬¦æ‰è€ƒè™‘åˆ‡åˆ†
        guard currentText.count >= minContentLength else {
            return // å†…å®¹å¤ªçŸ­ï¼Œä¸è¿›è¡Œåˆ‡åˆ†
        }
        
        // æ£€æŸ¥æœ€å°æ—¶é•¿é™åˆ¶ - 5ç§’å¼ºåˆ¶è¦æ±‚
        let currentDuration = Date().timeIntervalSince(currentSegmentStartTime)
        guard currentDuration >= 5.0 else {
            return // æœªè¾¾åˆ°æœ€å°5ç§’æ—¶é•¿ï¼Œä¸å…è®¸åˆ†æ®µ
        }
        
        // 1. å¥å­å®Œæ•´æ€§æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œç«‹å³è§¦å‘ï¼‰
        if detectSentenceEnd(currentText) {
            // è¦æ±‚è¶³å¤Ÿçš„å†…å®¹é•¿åº¦
            if currentText.count >= 100 {
                shouldCreateSegment = true
                segmentReason = "sentence completion (5s+ duration)"
            }
        }
        
        // 2. è¯­æ³•å®Œæ•´çš„ä»å¥æˆ–çŸ­è¯­
        else if detectGrammaticalPause(currentText) && currentText.count >= 100 {
            shouldCreateSegment = true
            segmentReason = "grammatical pause (5s+ duration)"
        }
        
        // 3. SFSpeechRecognizeræ ‡è®°ä¸ºfinalï¼ˆæ›´ä¸¥æ ¼æ¡ä»¶ï¼‰
        else if isFinal && currentText.count >= 100 {
            shouldCreateSegment = true
            segmentReason = "speech recognizer final (5s+ duration)"
        }
        
        // 4. è¯­ä¹‰æ„ä¹‰å•å…ƒæ£€æµ‹ï¼ˆè¦æ±‚æ›´é•¿å†…å®¹ï¼‰
        else if detectSemanticUnit(currentText) && currentText.count >= 100 {
            shouldCreateSegment = true
            segmentReason = "semantic unit complete (5s+ duration)"
        }
        
        if shouldCreateSegment {
            print("ğŸ¯ SEMANTIC: Creating segment - Reason: \(segmentReason)")
            createSemanticSegment()
        }
    }
    
    // åœé¡¿åŸºç¡€çš„è¾…åŠ©åˆ‡åˆ†ï¼ˆå¤‡ç”¨æœºåˆ¶ï¼‰
    private func checkForPauseBasedSegmentation(_ currentText: String) {
        let timeSinceLastChange = Date().timeIntervalSince(lastSignificantUpdate)
        
        // æ£€æŸ¥æœ€å°æ—¶é•¿é™åˆ¶
        let currentDuration = Date().timeIntervalSince(currentSegmentStartTime)
        guard currentDuration >= 5.0 else {
            return // æœªè¾¾åˆ°æœ€å°5ç§’æ—¶é•¿ï¼Œä¸å…è®¸åˆ†æ®µ
        }
        
        // åˆç†çš„åœé¡¿æ—¶é—´é˜ˆå€¼ï¼Œé¿å…é¢‘ç¹åˆ†æ®µ
        if timeSinceLastChange >= 6.0 && !currentText.isEmpty {
            // è¦æ±‚è¶³å¤Ÿçš„å†…å®¹é•¿åº¦
            if currentText.count >= 100 && !hasRecentSegment() {
                print("â¸ï¸ PAUSE: Creating segment due to extended pause (5s+ duration)")
                createSemanticSegment()
            }
        }
        
        // å¼ºåˆ¶åˆ‡åˆ†ï¼ˆé¿å…æé•¿æ®µè½ï¼‰
        if timeSinceLastChange >= maxSegmentDuration {
            print("âš¡ FORCE: Creating segment due to max duration")
            createSemanticSegment()
        }
    }
    
    // å¢å¼ºçš„å¥å­ç»“æŸæ£€æµ‹
    private func detectSentenceEnd(_ text: String) -> Bool {
        let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !trimmedText.isEmpty else { return false }
        
        // ä¸­è‹±æ–‡å¥å­ç»“æŸæ ‡å¿—
        let sentenceEnders = [".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ"]
        
        // æ£€æŸ¥æ˜¯å¦ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾ï¼ˆåŒ…æ‹¬åé¢å¯èƒ½æœ‰ç©ºæ ¼ï¼‰
        for ender in sentenceEnders {
            if trimmedText.hasSuffix(ender) {
                // ç¡®ä¿ä¸æ˜¯ç¼©å†™è¯ï¼ˆå¦‚Dr. Mr. etc.ï¼‰
                if !isAbbreviation(beforePunctuation: ender, in: trimmedText) {
                    return true
                }
            }
        }
        
        // æ£€æŸ¥çœç•¥å·
        if trimmedText.hasSuffix("...") || trimmedText.hasSuffix("â€¦") {
            return true
        }
        
        return false
    }
    
    // è¯­æ³•åœé¡¿æ£€æµ‹ï¼ˆä»å¥ã€çŸ­è¯­è¾¹ç•Œï¼‰
    private func detectGrammaticalPause(_ text: String) -> Bool {
        let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard trimmedText.count >= 10 else { return false }
        
        // æ£€æµ‹å¸¸è§çš„è¯­æ³•åœé¡¿ç‚¹
        let pausePatterns = [
            // è‹±æ–‡è¯­æ³•åœé¡¿
            ", and ", ", but ", ", so ", ", because ", ", although ", ", however ",
            ", while ", ", when ", ", where ", ", which ", ", that ",
            // ä¸­æ–‡è¯­æ³•åœé¡¿  
            "ï¼Œè€Œä¸”", "ï¼Œä½†æ˜¯", "ï¼Œæ‰€ä»¥", "ï¼Œå› ä¸º", "ï¼Œè™½ç„¶", "ï¼Œç„¶è€Œ",
            "ï¼Œå½“", "ï¼Œåœ¨", "ï¼Œå¦‚æœ", "ï¼Œé™¤é", "ï¼Œç›´åˆ°"
        ]
        
        for pattern in pausePatterns {
            if trimmedText.contains(pattern) {
                // æ£€æŸ¥åœé¡¿åæ˜¯å¦æœ‰å®Œæ•´çš„ä»å¥
                let components = trimmedText.components(separatedBy: pattern)
                if components.count >= 2 && components.last?.count ?? 0 >= 5 {
                    return true
                }
            }
        }
        
        return false
    }
    
    // è¯­ä¹‰å•å…ƒæ£€æµ‹
    private func detectSemanticUnit(_ text: String) -> Bool {
        let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard trimmedText.count >= 20 else { return false }
        
        // æ£€æµ‹å®Œæ•´çš„è¯­ä¹‰å•å…ƒæ ‡å¿—
        let semanticMarkers = [
            // è‹±æ–‡è¯­ä¹‰è¾¹ç•Œ
            " first ", " second ", " third ", " finally ", " in conclusion ",
            " moreover ", " furthermore ", " on the other hand ", " in addition ",
            // ä¸­æ–‡è¯­ä¹‰è¾¹ç•Œ
            "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "æ€»ä¹‹", "å¦å¤–", "æ­¤å¤–", "å¦ä¸€æ–¹é¢"
        ]
        
        for marker in semanticMarkers {
            if trimmedText.lowercased().contains(marker.lowercased()) {
                return true
            }
        }
        
        // æ£€æµ‹é—®ç­”å¯¹è¯æ¨¡å¼
        if trimmedText.contains("?") && trimmedText.contains(".") {
            return true
        }
        
        return false
    }
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™è¯
    private func isAbbreviation(beforePunctuation punct: String, in text: String) -> Bool {
        guard let range = text.range(of: punct, options: .backwards) else { return false }
        let beforePunct = String(text[..<range.lowerBound])
        
        let abbreviations = ["Dr", "Mr", "Mrs", "Ms", "Prof", "etc", "vs", "Inc", "Ltd"]
        
        for abbr in abbreviations {
            if beforePunct.hasSuffix(abbr) {
                return true
            }
        }
        
        return false
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„æ®µè½
    private func hasRecentSegment() -> Bool {
        guard let lastSegment = realtimeSegments.last else { return false }
        return Date().timeIntervalSince(lastSegment.timestamp) < 12.0  // ä»10ç§’æå‡åˆ°12ç§’
    }
    
    private func determineQualityFromConfidence(_ confidence: Float) -> TranscriptionQuality {
        switch confidence {
        case 0.9...1.0: return .excellent
        case 0.7..<0.9: return .good
        case 0.5..<0.7: return .fair
        default: return .poor
        }
    }
    
    private func restartRecognitionSession() {
        // é¿å…è¿‡äºé¢‘ç¹çš„é‡å¯
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            if self.isTranscribing {
                self.recognitionRequest?.endAudio()
                self.recognitionTask?.cancel()
                
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
                    if self.isTranscribing {
                        self.setupRecognitionRequest()
                    }
                }
            }
        }
    }
    
    // MARK: - Quality Metrics
    private func updateAudioQualityMetrics(_ quality: AudioQualityMetrics) {
        // ç®€åŒ–çš„è´¨é‡æ›´æ–°
        if quality.signalToNoiseRatio > 20 {
            transcriptionQuality = .excellent
        } else if quality.signalToNoiseRatio > 10 {
            transcriptionQuality = .good
        } else if quality.signalToNoiseRatio > 5 {
            transcriptionQuality = .fair
        } else {
            transcriptionQuality = .poor
        }
    }
    
    // MARK: - Timer Management
    private func startTranscriptionTimer() {
        transcriptionTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { _ in
            // å®šæœŸæ£€æŸ¥è½¬å†™çŠ¶æ€
            if self.isTranscribing && !self.currentTranscript.isEmpty {
                // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é¢å¤–çš„å¤„ç†é€»è¾‘
            }
        }
    }
    
    private func startSegmentTimer() {
        print("ğŸ”¥ Starting semantic check timer with interval: \(segmentInterval) seconds")
        segmentTimer?.invalidate() // ç¡®ä¿ä¹‹å‰çš„å®šæ—¶å™¨è¢«æ¸…ç†
        
        DispatchQueue.main.async {
            self.segmentTimer = Timer.scheduledTimer(withTimeInterval: self.segmentInterval, repeats: true) { timer in
                // å®šæ—¶å™¨ç°åœ¨ä¸»è¦ç”¨äºè¯­ä¹‰æ£€æµ‹çš„è¾…åŠ©æ£€æŸ¥
                self.performSemanticCheck()
            }
            
            if let timer = self.segmentTimer {
                RunLoop.main.add(timer, forMode: .common)
                print("âœ… Semantic check timer started successfully")
            } else {
                print("âŒ Failed to create semantic check timer")
            }
        }
    }
    
    // å®šæœŸè¯­ä¹‰æ£€æŸ¥ï¼ˆè¾…åŠ©è§¦å‘ï¼‰
    private func performSemanticCheck() {
        guard isTranscribing else { return }
        
        let timeSinceLastChange = Date().timeIntervalSince(lastSignificantUpdate)
        
        // å¦‚æœå¾ˆä¹…æ²¡æœ‰å˜åŒ–ï¼Œä½†æœ‰å†…å®¹ï¼Œå¯èƒ½éœ€è¦å¼ºåˆ¶åˆ†æ®µ
        if timeSinceLastChange >= 4.0 && !currentTranscript.isEmpty {
            let hasSignificantContent = currentTranscript.count > 10
            let noRecentSegments = realtimeSegments.isEmpty || 
                Date().timeIntervalSince(realtimeSegments.last?.timestamp ?? Date()) > 5.0
            
            if hasSignificantContent && noRecentSegments {
                print("â° Timer triggered semantic segmentation due to inactivity")
                createSemanticSegment()
            }
        }
    }
    
    private func createTimedSegment() {
        print("ğŸ”¥ createTimedSegment called, isTranscribing: \(self.isTranscribing), segments count: \(self.realtimeSegments.count)")
        
        DispatchQueue.main.async {
            // åªæœ‰åœ¨æ­£åœ¨è½¬å½•æ—¶æ‰å¤„ç†
            guard self.isTranscribing else {
                print("âŒ Not transcribing, skip segment creation")
                return
            }
            
            print("â° Timer triggered - creating segment...")
            print("ğŸ“ Current transcript: '\(self.currentTranscript)'")
            print("ğŸ¯ Current segments count: \(self.realtimeSegments.count)")
            
            // æå–è¿™ä¸€æ®µçš„æ–°å†…å®¹ï¼ˆå»é™¤ç´¯åŠ ï¼‰
            let segmentText = self.currentTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
            let previousCombinedText = self.realtimeSegments.map { $0.text }.joined(separator: " ")
            let newText = self.extractNewContent(from: segmentText, excluding: previousCombinedText)
            let finalText = newText.isEmpty ? "[No new speech]" : newText
            
            // ä¸ºå½“å‰æ–‡æœ¬ç”Ÿæˆç¿»è¯‘
            let translation = self.generateTranslationForText(finalText)
            
            let segment = RealtimeSegment(
                id: UUID(),
                text: finalText,
                timestamp: self.currentSegmentStartTime,
                confidence: self.confidence,
                language: self.detectedLanguage,
                quality: self.transcriptionQuality,
                duration: Date().timeIntervalSince(self.currentSegmentStartTime),
                translation: translation
            )
            
            self.realtimeSegments.append(segment)
            print("âœ… Created timed segment #\(self.realtimeSegments.count): '\(finalText)' at \(self.currentSegmentStartTime)")
            print("ğŸ“Š New content only: '\(finalText)'")
            
            // å¼€å§‹æ–°çš„æ—¶é—´æ®µï¼Œä½†ä¿ç•™å½“å‰è½¬å½•å†…å®¹ä½œä¸ºåŸºç¡€
            self.currentSegmentStartTime = Date()
            print("ğŸ”„ New segment started at: \(self.currentSegmentStartTime)")
        }
    }
    
    // MARK: - Language Management
    func switchLanguage(to languageCode: String) {
        guard supportedLanguages.contains(languageCode) else { return }
        
        let wasTranscribing = isTranscribing
        
        if wasTranscribing {
            stopRealTimeTranscription()
        }
        
        detectedLanguage = languageCode
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: languageCode))
        speechRecognizer?.delegate = self
        
        if wasTranscribing {
            Task {
                try? await startRealTimeTranscription()
            }
        }
    }
    
    // MARK: - Export Functions
    @MainActor
    func exportRealtimeSegments() -> [PlaybackSegment] {
        // è®¡ç®—å½•éŸ³å¼€å§‹æ—¶é—´ï¼ˆç¬¬ä¸€ä¸ªsegmentçš„æ—¶é—´æˆ³ï¼‰
        let recordingStartTime = realtimeSegments.first?.timestamp ?? Date()
        
        return realtimeSegments.map { segment in
            // è®¡ç®—ç›¸å¯¹äºå½•éŸ³å¼€å§‹çš„æ—¶é—´ï¼ˆç§’ï¼‰
            let relativeStartTime = segment.timestamp.timeIntervalSince(recordingStartTime)
            let relativeEndTime = relativeStartTime + segment.duration
            
            var playbackSegment = PlaybackSegment(
                startTime: relativeStartTime,
                endTime: relativeEndTime,
                transcription: segment.text,
                translation: segment.translation.isEmpty ? generateTranslationForText(segment.text) : segment.translation // ä½¿ç”¨å·²ä¿å­˜çš„ç¿»è¯‘
            )
            
            // è®¾ç½®è´¨é‡ä¿¡æ¯
            playbackSegment.confidence = segment.confidence
            playbackSegment.language = segment.language
            playbackSegment.transcriptionQuality = segment.quality
            playbackSegment.audioQualityMetrics = AudioQualityMetrics(
                signalToNoiseRatio: 15.0, // é»˜è®¤å€¼
                averageAmplitude: 0.1,
                peakAmplitude: 0.3,
                spectralCentroid: 2000,
                timestamp: segment.timestamp
            )
            
            return playbackSegment
        }
    }
    
    func clearSegments() {
        realtimeSegments.removeAll()
        currentTranscript = ""
    }
    
    // MARK: - AIé©±åŠ¨çš„æ™ºèƒ½åˆ†æ®µä¼˜åŒ–
    
    /// AIé©±åŠ¨çš„åˆ†æ®µä¼˜åŒ– - è°ƒç”¨é€šä¹‰åƒé—®è¯­ä¹‰åˆ†æ
    @MainActor
    func performAISegmentOptimization() async {
        guard realtimeSegments.count > 1 else { return }
        
        print("ğŸ§  Starting AI-driven segment optimization for \(realtimeSegments.count) segments")
        
        do {
            // 1. è½¬æ¢ä¸ºPlaybackSegmentè¿›è¡Œåˆ†æ
            let playbackSegments = realtimeSegments.map { segment in
                PlaybackSegment(
                    startTime: segment.timestamp.timeIntervalSince1970,
                    endTime: segment.timestamp.timeIntervalSince1970 + segment.duration,
                    transcription: segment.text,
                    translation: segment.translation ?? ""
                )
            }
            
            // 2. è·å–AIåˆ†æ®µåˆå¹¶å»ºè®®
            let mergeRecommendations = QianwenSemanticManager.shared.getSegmentMergeRecommendations(playbackSegments)
            
            print("ğŸ¤– AI Analysis: \(mergeRecommendations.recommendedMerges.count) merge groups identified")
            
            // 3. åº”ç”¨AIå»ºè®®è¿›è¡Œæ™ºèƒ½åˆå¹¶
            var optimizedSegments: [RealtimeSegment] = []
            var processedIndices: Set<Int> = []
            
            for mergeGroup in mergeRecommendations.recommendedMerges {
                // æ‰¾åˆ°å¯¹åº”çš„åŸå§‹åˆ†æ®µ
                var groupSegments: [RealtimeSegment] = []
                
                for segment in mergeGroup.segments {
                    if let index = realtimeSegments.firstIndex(where: { $0.id == segment.id }) {
                        if !processedIndices.contains(index) {
                            groupSegments.append(realtimeSegments[index])
                            processedIndices.insert(index)
                        }
                    }
                }
                
                if !groupSegments.isEmpty {
                    let mergedSegment = createAIOptimizedSegment(from: groupSegments, aiReason: mergeGroup.reason)
                    optimizedSegments.append(mergedSegment)
                    print("ğŸ”— AI merged \(groupSegments.count) segments: \(mergeGroup.reason)")
                }
            }
            
            // 4. æ·»åŠ æœªè¢«åˆå¹¶çš„å•ç‹¬åˆ†æ®µ
            for (index, segment) in realtimeSegments.enumerated() {
                if !processedIndices.contains(index) {
                    optimizedSegments.append(segment)
                }
            }
            
            // 5. æŒ‰æ—¶é—´æˆ³æ’åº
            optimizedSegments.sort { $0.timestamp < $1.timestamp }
            
            // 6. æ›´æ–°åˆ†æ®µåˆ—è¡¨
            let originalCount = realtimeSegments.count
            realtimeSegments = optimizedSegments
            
            print("ğŸ¯ AI optimization completed: \(originalCount) â†’ \(realtimeSegments.count) segments")
            
        } catch {
            print("âŒ AI segment optimization failed: \(error)")
            // ä¿æŒåŸæœ‰åˆ†æ®µä½œä¸ºåå¤‡æ–¹æ¡ˆ
        }
    }
    
    /// åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå½“å‰åˆå¹¶ç»„
    private func shouldEndMergeGroup(currentGroup: [RealtimeSegment], nextSegment: RealtimeSegment?) -> Bool {
        guard currentGroup.first != nil else { return true }
        
        // 1. åˆå¹¶ç»„å·²ç»è¶³å¤Ÿé•¿ï¼ˆæ—¶é—´æˆ–æ–‡æœ¬ï¼‰
        let totalDuration = currentGroup.reduce(0) { $0 + $1.duration }
        let totalTextLength = currentGroup.map { $0.text }.joined(separator: " ").count
        
        if totalDuration >= 10.0 || totalTextLength >= 100 {
            return true
        }
        
        // 2. æ£€æŸ¥è¯­ä¹‰è¿è´¯æ€§
        if let next = nextSegment {
            let currentText = currentGroup.map { $0.text }.joined(separator: " ")
            let semanticGap = calculateSemanticGap(between: currentText, and: next.text)
            
            // è¯­ä¹‰å·®è·å¤ªå¤§ï¼Œåº”è¯¥åˆ†å¼€
            if semanticGap > 0.7 {
                return true
            }
        }
        
        // 3. æ—¶é—´é—´éš”æ£€æŸ¥
        if let lastSegment = currentGroup.last, let next = nextSegment {
            let timeGap = next.timestamp.timeIntervalSince(lastSegment.timestamp + lastSegment.duration)
            
            // è¶…è¿‡3ç§’é—´éš”ï¼Œåº”è¯¥åˆ†å¼€
            if timeGap > 3.0 {
                return true
            }
        }
        
        // 4. å¥å­å®Œæ•´æ€§æ£€æŸ¥
        let lastText = currentGroup.last?.text ?? ""
        if detectSentenceEnd(lastText) && currentGroup.count >= 2 {
            return true
        }
        
        return false
    }
    
    /// è®¡ç®—ä¸¤æ®µæ–‡æœ¬é—´çš„è¯­ä¹‰å·®è·
    private func calculateSemanticGap(between text1: String, and text2: String) -> Double {
        // ç®€åŒ–çš„è¯­ä¹‰å·®è·è®¡ç®—
        let words1 = Set(text1.lowercased().components(separatedBy: .whitespacesAndNewlines))
        let words2 = Set(text2.lowercased().components(separatedBy: .whitespacesAndNewlines))
        
        let intersection = words1.intersection(words2)
        let union = words1.union(words2)
        
        // è¿”å›è¯­ä¹‰å·®è·ï¼ˆ1.0 - ç›¸ä¼¼åº¦ï¼‰
        if union.isEmpty {
            return 1.0
        }
        
        let similarity = Double(intersection.count) / Double(union.count)
        return 1.0 - similarity
    }
    
    /// åˆ›å»ºAIä¼˜åŒ–çš„åˆå¹¶åˆ†æ®µ
    private func createAIOptimizedSegment(from segments: [RealtimeSegment], aiReason: String) -> RealtimeSegment {
        guard let firstSegment = segments.first else {
            fatalError("Cannot merge empty segment group")
        }
        
        if segments.count == 1 {
            return firstSegment
        }
        
        // åˆå¹¶æ–‡æœ¬
        let mergedText = segments.map { $0.text }.joined(separator: " ")
        
        // è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        let avgConfidence = segments.map { $0.confidence }.reduce(0, +) / Float(segments.count)
        
        // è®¡ç®—æ€»æ—¶é•¿
        let totalDuration = segments.reduce(0) { $0 + $1.duration }
        
        // åˆå¹¶ç¿»è¯‘
        let mergedTranslation = segments.compactMap { $0.translation }
            .filter { !$0.isEmpty }
            .joined(separator: " ")
        
        // ç¡®å®šè´¨é‡ï¼ˆå–æœ€å¥½çš„ï¼‰
        let qualityScores = segments.map { quality -> Int in
            switch quality.quality {
            case .excellent: return 4
            case .good: return 3
            case .fair: return 2
            case .poor: return 1
            case .unknown: return 1
            }
        }
        let maxScore = qualityScores.max() ?? 2
        let bestQuality: TranscriptionQuality = {
            switch maxScore {
            case 4: return .excellent
            case 3: return .good
            case 2: return .fair
            default: return .poor
            }
        }()
        
        return RealtimeSegment(
            id: UUID(),
            text: mergedText,
            timestamp: firstSegment.timestamp,
            confidence: avgConfidence,
            language: firstSegment.language,
            quality: bestQuality,
            duration: totalDuration,
            translation: mergedTranslation.isEmpty ? "" : mergedTranslation
        )
    }
    
    // å¼ºåˆ¶åˆ›å»ºæ®µè½ï¼ˆå¤–éƒ¨è°ƒç”¨ï¼‰
    func forceCreateSegment() {
        createSemanticSegment()
    }
    
    // å¤‡ç”¨çš„å®šæ—¶åˆ‡åˆ†æ–¹æ³•ï¼ˆä¿ç•™ï¼‰
    private func createTimedSegmentBackup() {
        print("ğŸ”¥ createTimedSegmentBackup called (fallback method)")
        
        DispatchQueue.main.async {
            guard self.isTranscribing else {
                print("âŒ Not transcribing, skip backup segment creation")
                return
            }
            
            let segmentText = self.currentTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
            let previousCombinedText = self.realtimeSegments.map { $0.text }.joined(separator: " ")
            let newText = self.extractNewContent(from: segmentText, excluding: previousCombinedText)
            let finalText = newText.isEmpty ? "[No new speech]" : newText
            
            let translation = self.generateTranslationForText(finalText)
            
            let segment = RealtimeSegment(
                id: UUID(),
                text: finalText,
                timestamp: self.currentSegmentStartTime,
                confidence: self.confidence,
                language: self.detectedLanguage,
                quality: self.transcriptionQuality,
                duration: Date().timeIntervalSince(self.currentSegmentStartTime),
                translation: translation
            )
            
            self.realtimeSegments.append(segment)
            print("âœ… Created backup timed segment #\(self.realtimeSegments.count): '\(finalText)'")
            
            self.currentSegmentStartTime = Date()
            print("ğŸ”„ New backup segment started at: \(self.currentSegmentStartTime)")
        }
    }
    
    // æå–æ–°å†…å®¹ï¼Œå»é™¤ç´¯åŠ çš„é‡å¤éƒ¨åˆ†
    private func extractNewContent(from currentText: String, excluding previousText: String) -> String {
        // å¦‚æœæ²¡æœ‰ä¹‹å‰çš„å†…å®¹ï¼Œè¿”å›å½“å‰å…¨éƒ¨å†…å®¹
        guard !previousText.isEmpty else {
            return currentText
        }
        
        // ç§»é™¤å‰ç¼€ä¸­å·²å­˜åœ¨çš„éƒ¨åˆ†
        if currentText.hasPrefix(previousText) {
            let startIndex = currentText.index(currentText.startIndex, offsetBy: previousText.count)
            return String(currentText[startIndex...]).trimmingCharacters(in: .whitespacesAndNewlines)
        }
        
        // å¯»æ‰¾é‡å éƒ¨åˆ†å¹¶ç§»é™¤
        let currentWords = currentText.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty }
        let previousWords = previousText.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty }
        
        // ä»åå¾€å‰æ‰¾é‡å 
        var newWords: [String] = []
        var foundOverlap = false
        
        for i in 0..<currentWords.count {
            let word = currentWords[i]
            if !foundOverlap && previousWords.contains(word) {
                // æ‰¾åˆ°é‡å çš„èµ·å§‹ç‚¹ï¼Œè·³è¿‡å·²æœ‰çš„è¯
                continue
            }
            foundOverlap = true
            newWords.append(word)
        }
        
        return newWords.joined(separator: " ")
    }
    
    // ä¸ºå®æ—¶è½¬å½•æ–‡æœ¬ç”Ÿæˆç¿»è¯‘ï¼ˆä¿®å¤ç‰ˆï¼‰
    @MainActor
    private func generateTranslationForText(_ text: String) -> String {
        // æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©ºæˆ–å ä½ç¬¦
        if text.isEmpty || text.contains("[No") || text.contains("No new speech") {
            return ""
        }
        
        // æ£€æŸ¥API Keyæ˜¯å¦é…ç½®
        guard APIKeyManager.shared.hasQianwenKey else {
            return "Please configure Qianwen API key in Settings"
        }
        
        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸è¿”å›å‡çš„"Translating..."ï¼Œç›´æ¥è¿›è¡ŒçœŸå®ç¿»è¯‘
        let sourceLanguage = QianwenTranslateManager.shared.detectLanguage(text)
        let targetLanguage = sourceLanguage == "zh" ? "en" : "zh"
        
        // æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå‰ä¸€æ®µå†…å®¹ï¼‰
        let context = buildTranslationContext()
        
        // ğŸ”¥ ç›´æ¥è¿›è¡ŒåŒæ­¥ç¿»è¯‘ï¼ˆæ”¹ä¸ºå¼‚æ­¥ä½†ç«‹å³è§¦å‘ï¼‰
        Task {
            do {
                let translation = try await QianwenTranslateManager.shared.translateWithContext(
                    text,
                    context: context,
                    from: sourceLanguage, 
                    to: targetLanguage
                )
                
                print("âœ… Translation completed: '\(text)' -> '\(translation)'")
                
                // ç«‹å³æ›´æ–°å¯¹åº”segmentçš„ç¿»è¯‘
                await MainActor.run {
                    updateSegmentTranslation(originalText: text, translation: translation)
                }
            } catch {
                print("âŒ Translation failed: \(error)")
                await MainActor.run {
                    updateSegmentTranslation(originalText: text, translation: "Translation error: \(error.localizedDescription)")
                }
            }
        }
        
        // ğŸ”¥ è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œç­‰å¾…å¼‚æ­¥æ›´æ–°
        // è¿™æ ·ä¸ä¼šæ˜¾ç¤ºå‡çš„"Translating..."ï¼Œè€Œæ˜¯ç©ºç™½ç›´åˆ°çœŸæ­£çš„ç¿»è¯‘å®Œæˆ
        return ""
    }
    
    // æ„å»ºç¿»è¯‘ä¸Šä¸‹æ–‡
    private func buildTranslationContext() -> String {
        // è·å–æœ€è¿‘çš„2-3ä¸ªæ®µè½ä½œä¸ºä¸Šä¸‹æ–‡
        let recentSegments = Array(realtimeSegments.suffix(3))
        let contextTexts = recentSegments.map { $0.text }
        
        if contextTexts.isEmpty {
            return ""
        }
        
        return "Previous context: " + contextTexts.joined(separator: " ")
    }
    
    // æ›´æ–°segmentçš„ç¿»è¯‘
    private func updateSegmentTranslation(originalText: String, translation: String) {
        if let index = realtimeSegments.firstIndex(where: { $0.text == originalText }) {
            realtimeSegments[index].translation = translation
        }
    }
    
    private func createSemanticSegment() {
        print("ğŸ”¥ createSemanticSegment called, isTranscribing: \(self.isTranscribing), segments count: \(self.realtimeSegments.count)")
        
        DispatchQueue.main.async {
            // åªæœ‰åœ¨æ­£åœ¨è½¬å½•æ—¶æ‰å¤„ç†
            guard self.isTranscribing else {
                print("âŒ Not transcribing, skip segment creation")
                return
            }
            
            let currentText = self.currentTranscript.trimmingCharacters(in: .whitespacesAndNewlines)
            
            // ğŸ”¥ ç®€åŒ–é€»è¾‘ï¼šä½¿ç”¨å­—ç¬¦ä½ç½®è¿½è¸ª
            let startPosition = self.lastProcessedLength
            let newContent: String
            
            if startPosition < currentText.count {
                let startIndex = currentText.index(currentText.startIndex, offsetBy: startPosition)
                newContent = String(currentText[startIndex...]).trimmingCharacters(in: .whitespacesAndNewlines)
            } else {
                newContent = ""
            }
            
            // ğŸ”¥ é™ä½æ–°å†…å®¹æœ€å°é•¿åº¦è¦æ±‚åˆ°10å­—ç¬¦
            guard !newContent.isEmpty && newContent.count >= 10 else {
                print("ğŸ“ New content too short: '\(newContent)' (length: \(newContent.count))")
                return
            }
            
            print("ğŸ¯ NEW content: '\(newContent)'")
            print("ğŸ“Š Full transcript: '\(currentText)'")
            print("ğŸ”¢ Previous length: \(self.lastProcessedLength), Current: \(currentText.count)")
            
            // ä¸ºæ–°å†…å®¹ç”Ÿæˆç¿»è¯‘
            let translation = self.generateTranslationForText(newContent)
            
            let segment = RealtimeSegment(
                id: UUID(),
                text: newContent,
                timestamp: self.currentSegmentStartTime,
                confidence: self.confidence,
                language: self.detectedLanguage,
                quality: self.transcriptionQuality,
                duration: Date().timeIntervalSince(self.currentSegmentStartTime),
                translation: translation
            )
            
            self.realtimeSegments.append(segment)
            print("âœ… Created semantic segment #\(self.realtimeSegments.count): '\(newContent)'")
            
            // ğŸ”¥ æ›´æ–°å¤„ç†ä½ç½®
            self.lastProcessedLength = currentText.count
            
            // é‡ç½®æ—¶é—´æ®µ
            self.currentSegmentStartTime = Date()
            self.lastSignificantUpdate = Date()
            self.lastTranscriptLength = currentText.count
            print("ğŸ”„ Updated processed length to: \(self.lastProcessedLength)")
        }
    }
    
    // MARK: - Hybrid Segmentation Algorithm (Industry Best Practice)
    
    private func processTranscriptionWithHybridSegmentation(_ result: SFSpeechRecognitionResult) {
        let transcriptText = result.bestTranscription.formattedString
        currentTranscript = transcriptText
        
        // æ›´æ–°ç½®ä¿¡åº¦å’Œè´¨é‡
        let avgConfidence = result.bestTranscription.segments.map { $0.confidence }.reduce(0, +) / Float(result.bestTranscription.segments.count)
        confidence = avgConfidence
        transcriptionQuality = determineQualityFromConfidence(avgConfidence)
        
        // æ·»åŠ åˆ°æ»‘åŠ¨çª—å£ç¼“å†²åŒº
        let bufferEntry = (text: transcriptText, timestamp: Date(), confidence: avgConfidence)
        audioSegmentBuffer.append(bufferEntry)
        
        // ç»´æŠ¤ç¼“å†²åŒºå¤§å°ï¼ˆä¿æŒæœ€è¿‘30ç§’çš„æ•°æ®ï¼‰
        let bufferTimeLimit = TimeInterval(30)
        audioSegmentBuffer = audioSegmentBuffer.filter { 
            Date().timeIntervalSince($0.timestamp) <= bufferTimeLimit 
        }
        
        // å¤šä¿¡å·è¾¹ç•Œæ£€æµ‹
        performMultiSignalBoundaryDetection(transcriptText, result: result)
        
        // æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ›å»ºåˆ†æ®µ
        if shouldCreateHybridSegment() {
            createHybridSegment()
        }
    }
    
    // å¤šä¿¡å·è¾¹ç•Œæ£€æµ‹ - ç»“åˆå£°å­¦å’Œè¯­ä¹‰ç‰¹å¾
    private func performMultiSignalBoundaryDetection(_ text: String, result: SFSpeechRecognitionResult) {
        // 1. å£°å­¦è¾¹ç•Œæ£€æµ‹ (åŸºäºWebRTC VADæ€æƒ³)
        detectAcousticBoundary(result)
        
        // 2. è¯­ä¹‰è¾¹ç•Œæ£€æµ‹ (åŸºäºBERTå’Œè¯­è¨€æ¨¡å‹ç ”ç©¶)
        detectSemanticBoundary(text)
        
        // 3. æ—¶é—´çª—å£æ£€æµ‹ (åŸºäºE2E Segmenter)
        detectTemporalBoundary()
    }
    
    // å£°å­¦è¾¹ç•Œæ£€æµ‹
    private func detectAcousticBoundary(_ result: SFSpeechRecognitionResult) {
        acousticBoundaryDetected = false
        
        // æ£€æµ‹è¯­éŸ³åœé¡¿æ¨¡å¼
        let segments = result.bestTranscription.segments
        if segments.count >= 2 {
            let lastSegment = segments[segments.count - 1]
            let secondLastSegment = segments[segments.count - 2]
            
            // è®¡ç®—æ®µè½é—´éš”
            let pauseDuration = lastSegment.timestamp - (secondLastSegment.timestamp + secondLastSegment.duration)
            
            // è¶…è¿‡é˜ˆå€¼è®¤ä¸ºæ˜¯å£°å­¦è¾¹ç•Œ
            if pauseDuration >= 0.8 { // 800msåœé¡¿
                acousticBoundaryDetected = true
                print("ğŸ”Š Acoustic boundary detected: pause = \\(pauseDuration)s")
            }
        }
        
        // æ£€æµ‹è¯­éŸ³ç»“æŸï¼ˆfinal resultï¼‰
        if result.isFinal {
            acousticBoundaryDetected = true
            print("ğŸ”Š Acoustic boundary: final result")
        }
    }
    
    // è¯­ä¹‰è¾¹ç•Œæ£€æµ‹
    private func detectSemanticBoundary(_ text: String) {
        semanticBoundaryDetected = false
        
        // 1. å¥å­è¾¹ç•Œæ£€æµ‹ï¼ˆå¼ºä¿¡å·ï¼‰
        let sentenceEnders = [".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ"]
        let lastChar = String(text.suffix(1))
        if sentenceEnders.contains(lastChar) {
            semanticBoundaryDetected = true
            print("ğŸ“ Semantic boundary: sentence ending '\\(lastChar)'")
            return
        }
        
        // 2. è¯­æ³•è¾¹ç•Œæ£€æµ‹ï¼ˆä¸­ä¿¡å·ï¼‰
        let clauseEnders = [", and ", ", but ", ", however ", ", although ", ", because "]
        let clauseEndersZh = ["ï¼Œè€Œä¸”", "ï¼Œä½†æ˜¯", "ï¼Œç„¶è€Œ", "ï¼Œè™½ç„¶", "ï¼Œå› ä¸º"]
        let allClauseEnders = clauseEnders + clauseEndersZh
        
        for clauseEnder in allClauseEnders {
            if text.lowercased().hasSuffix(clauseEnder.lowercased()) {
                semanticBoundaryDetected = true
                print("ğŸ“ Semantic boundary: clause ending '\\(clauseEnder)'")
                return
            }
        }
        
        // 3. æ®µè½è¿æ¥è¯æ£€æµ‹ï¼ˆå¼±ä¿¡å·ï¼‰
        let transitionWords = ["first", "second", "finally", "in conclusion", "meanwhile", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "æ€»ä¹‹", "åŒæ—¶"]
        let currentWords = text.components(separatedBy: .whitespacesAndNewlines).suffix(3)
        
        for word in currentWords {
            if transitionWords.contains(word.lowercased()) {
                semanticBoundaryDetected = true
                print("ğŸ“ Semantic boundary: transition word '\\(word)'")
                return
            }
        }
    }
    
    // æ—¶é—´çª—å£æ£€æµ‹
    private func detectTemporalBoundary() {
        let timeSinceLastBoundary = Date().timeIntervalSince(lastBoundaryTime)
        
        // è¶…è¿‡æœ€å¤§æ®µè½é•¿åº¦å¼ºåˆ¶åˆ†æ®µ
        if timeSinceLastBoundary >= maxSegmentLength {
            acousticBoundaryDetected = true
            print("â° Temporal boundary: max length reached (\\(timeSinceLastBoundary)s)")
        }
    }
    
    // æ··åˆåˆ†æ®µå†³ç­–
    private func shouldCreateHybridSegment() -> Bool {
        let timeSinceLastBoundary = Date().timeIntervalSince(lastBoundaryTime)
        
        // æ¡ä»¶1ï¼šåŒæ—¶æ£€æµ‹åˆ°å£°å­¦å’Œè¯­ä¹‰è¾¹ç•Œï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        if acousticBoundaryDetected && semanticBoundaryDetected && timeSinceLastBoundary >= minSegmentLength {
            print("âœ… High confidence segmentation: acoustic + semantic")
            return true
        }
        
        // æ¡ä»¶2ï¼šå¼ºè¯­ä¹‰è¾¹ç•Œ + è¶³å¤Ÿæ—¶é—´ï¼ˆä¸­ç½®ä¿¡åº¦ï¼‰
        if semanticBoundaryDetected && timeSinceLastBoundary >= minSegmentLength * 1.2 {
            print("âœ… Medium confidence segmentation: strong semantic")
            return true
        }
        
        // æ¡ä»¶3ï¼šå£°å­¦è¾¹ç•Œ + è¾ƒé•¿æ—¶é—´ï¼ˆä½ç½®ä¿¡åº¦ï¼‰
        if acousticBoundaryDetected && timeSinceLastBoundary >= minSegmentLength * 2.0 {
            print("âœ… Low confidence segmentation: acoustic + time")
            return true
        }
        
        // æ¡ä»¶4ï¼šå¼ºåˆ¶åˆ†æ®µï¼ˆè¶…æ—¶ä¿æŠ¤ï¼‰
        if timeSinceLastBoundary >= maxSegmentLength {
            print("âš ï¸ Force segmentation: timeout protection")
            return true
        }
        
        return false
    }
    
    // åˆ›å»ºæ··åˆåˆ†æ®µ
    private func createHybridSegment() {
        DispatchQueue.main.async {
            guard self.isTranscribing else { return }
            
            // ä½¿ç”¨æ»‘åŠ¨çª—å£æå–å†…å®¹
            let segmentText = self.extractContentWithStride()
            
            guard !segmentText.isEmpty else {
                print("ğŸ“ No content to segment")
                return
            }
            
            print("ğŸ¯ Creating hybrid segment: '\\(segmentText)'")
            
            // åˆ›å»ºå®æ—¶åˆ†æ®µ
            self.createRealtimeSegment(with: segmentText)
            
            // é‡ç½®çŠ¶æ€
            self.resetSegmentationState()
        }
    }
    
    // ä½¿ç”¨æ­¥é•¿æå–å†…å®¹ï¼ˆWav2Vec2é£æ ¼ï¼‰
    private func extractContentWithStride() -> String {
        guard !audioSegmentBuffer.isEmpty else { return "" }
        
        // è·å–å½“å‰å¤„ç†çª—å£å†…çš„å†…å®¹
        let currentTime = Date()
        let windowStart = currentTime.addingTimeInterval(-windowSize)
        
        // æå–çª—å£å†…çš„æ–‡æœ¬
        let windowEntries = audioSegmentBuffer.filter { entry in
            entry.timestamp >= windowStart && entry.timestamp <= currentTime
        }
        
        if let latestEntry = windowEntries.last {
            let newContent = extractIncrementalContent(from: latestEntry.text)
            return newContent
        }
        
        return ""
    }
    
    // æå–å¢é‡å†…å®¹
    private func extractIncrementalContent(from fullText: String) -> String {
        let startPosition = lastProcessedLength
        
        if startPosition < fullText.count {
            let startIndex = fullText.index(fullText.startIndex, offsetBy: startPosition)
            let newContent = String(fullText[startIndex...]).trimmingCharacters(in: .whitespacesAndNewlines)
            
            // æ›´æ–°å·²å¤„ç†é•¿åº¦
            lastProcessedLength = fullText.count
            
            return newContent
        }
        
        return ""
    }
    
    // é‡ç½®åˆ†æ®µçŠ¶æ€
    private func resetSegmentationState() {
        acousticBoundaryDetected = false
        semanticBoundaryDetected = false
        lastBoundaryTime = Date()
        currentWindowBuffer = ""
        strideBuffer = ""
    }
    
    // åˆ›å»ºå®æ—¶åˆ†æ®µ
    @MainActor
    private func createRealtimeSegment(with text: String) {
        guard !text.isEmpty else { return }
        
        let currentTime = Date()
        let duration = currentTime.timeIntervalSince(currentSegmentStartTime)
        
        // ç”Ÿæˆç¿»è¯‘
        let translation = generateTranslationForText(text)
        
        // åˆ›å»ºæ–°åˆ†æ®µ
        let segment = RealtimeSegment(
            id: UUID(),
            text: text,
            timestamp: currentTime,
            confidence: confidence,
            language: "auto", // è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            quality: transcriptionQuality,
            duration: duration,
            translation: translation
        )
        
        // æ·»åŠ åˆ°æ•°ç»„
        realtimeSegments.append(segment)
        
        print("âœ… Created hybrid segment [\(String(format: "%.1f", duration))s]: '\(text)' -> '\(translation)'")
        
        // æ›´æ–°åˆ†æ®µå¼€å§‹æ—¶é—´ä¸ºå½“å‰æ—¶é—´
        currentSegmentStartTime = currentTime
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension RealTimeTranscriptionManager: SFSpeechRecognizerDelegate {
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        DispatchQueue.main.async {
            if !available && self.isTranscribing {
                self.transcriptionError = "Speech recognition became unavailable"
                self.stopRealTimeTranscription()
            }
        }
    }
}

// MARK: - Supporting Types
struct RealtimeSegment: Identifiable, Codable {
    let id: UUID
    let text: String
    let timestamp: Date
    let confidence: Float
    let language: String
    let quality: TranscriptionQuality
    let duration: TimeInterval
    var translation: String // æ·»åŠ ç¿»è¯‘å­—æ®µ
    
    init(id: UUID = UUID(), text: String, timestamp: Date, confidence: Float, language: String, quality: TranscriptionQuality, duration: TimeInterval, translation: String = "") {
        self.id = id
        self.text = text
        self.timestamp = timestamp
        self.confidence = confidence
        self.language = language
        self.quality = quality
        self.duration = duration
        self.translation = translation
    }
}

// TranscriptionQualityå’ŒAudioQualityMetricså·²åœ¨RecordingSessionManagerä¸­å®šä¹‰

struct TranscriptionQualityMetrics {
    let level: TranscriptionQuality
    let confidence: Float
    let wordAccuracy: Float
    let sentenceCompleteness: Float
}

// MARK: - Language Detection Engine
class LanguageDetectionEngine {
    func detectLanguage(from buffer: AVAudioPCMBuffer) async -> String {
        // å®ç°éŸ³é¢‘è¯­è¨€æ£€æµ‹é€»è¾‘
        // è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…å¯ä»¥é›†æˆMLæ¨¡å‹æˆ–äº‘æœåŠ¡
        return await withCheckedContinuation { continuation in
            DispatchQueue.global().asyncAfter(deadline: .now() + 0.1) {
                // æ¨¡æ‹Ÿè¯­è¨€æ£€æµ‹ç»“æœ
                let detectedLanguages = ["en", "zh", "es", "fr"]
                let randomLanguage = detectedLanguages.randomElement() ?? "en"
                continuation.resume(returning: randomLanguage)
            }
        }
    }
}

// MARK: - Transcription Quality Analyzer
class TranscriptionQualityAnalyzer {
    func analyzeAudioQuality(_ buffer: AVAudioPCMBuffer) -> AudioQualityMetrics {
        // åˆ†æéŸ³é¢‘è´¨é‡
        guard let channelData = buffer.floatChannelData?[0] else {
            return AudioQualityMetrics(signalToNoiseRatio: 0, averageAmplitude: 0, peakAmplitude: 0, spectralCentroid: 0, timestamp: Date())
        }
        
        let frameLength = Int(buffer.frameLength)
        var sum: Float = 0
        var peak: Float = 0
        
        for i in 0..<frameLength {
            let sample = abs(channelData[i])
            sum += sample
            peak = max(peak, sample)
        }
        
        let average = sum / Float(frameLength)
        let snr = peak > 0 ? 20 * log10(peak / (average + 0.001)) : 0
        
        return AudioQualityMetrics(
            signalToNoiseRatio: snr,
            averageAmplitude: average,
            peakAmplitude: peak,
            spectralCentroid: 0, // ç®€åŒ–å®ç°
            timestamp: Date()
        )
    }
    
    func analyzeTranscriptionQuality(_ result: SFSpeechRecognitionResult) -> TranscriptionQualityMetrics {
        let confidence = result.bestTranscription.segments.map { $0.confidence }.reduce(0, +) / Float(result.bestTranscription.segments.count)
        
        let level: TranscriptionQuality
        switch confidence {
        case 0.9...1.0: level = .excellent
        case 0.7..<0.9: level = .good
        case 0.5..<0.7: level = .fair
        case 0.0..<0.5: level = .poor
        default: level = .unknown
        }
        
        return TranscriptionQualityMetrics(
            level: level,
            confidence: confidence,
            wordAccuracy: confidence,
            sentenceCompleteness: calculateSentenceCompleteness(result.bestTranscription.formattedString)
        )
    }
    
    private func calculateSentenceCompleteness(_ text: String) -> Float {
        // ç®€å•çš„å¥å­å®Œæ•´æ€§åˆ†æ
        let sentences = text.components(separatedBy: CharacterSet(charactersIn: ".!?"))
        let completeSentences = sentences.filter { !$0.trimmingCharacters(in: .whitespaces).isEmpty }
        
        return sentences.isEmpty ? 0 : Float(completeSentences.count) / Float(sentences.count)
    }
} 